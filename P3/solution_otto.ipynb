{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the\n",
    "# First, we import necessary libraries:\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights, efficientnet_b0, efficientnet_b1, efficientnet_b2, efficientnet_b3, efficientnet_b7, efficientnet_b5\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.models import EfficientNet_B7_Weights\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import BinaryAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(path, batch_size, num_work):\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract\n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define transformations, standard as copied from the web, first resize to 224 x 224 image,\n",
    "    # then transform to tensor and normalize with magic numbers from the web.\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # pretrained_weights = EfficientNet_B7_Weights.IMAGENET1K_V1\n",
    "    # train_transforms = pretrained_weights.transforms()\n",
    "\n",
    "    # The train dataset, loaded in batches.\n",
    "    train_dataset = datasets.ImageFolder(root=\"P3/dataset/\", transform=train_transforms)\n",
    "\n",
    "    # We load all the data at once. Hence, a massive batch size.\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=num_work)\n",
    "\n",
    "    # Pretrained model resnet50, use the newest version with weights IMAGENET1K_V2, 80.858% accuracy.\n",
    "    # model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    # Pretrained model efficientnet_b0\n",
    "    # model = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Pretrained model efficientnet_b3\n",
    "    # model = efficientnet_b3(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Pretrained model efficientnet_b5\n",
    "    model = efficientnet_b5(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Pretrained model efficientnet_b7\n",
    "    # model = efficientnet_b7(weights='IMAGENET1K_V1')  \n",
    "\n",
    "    # Put model into evaluation mode to get the calculated values for each entry.\n",
    "    model.eval()\n",
    "\n",
    "    # Sent model to the gpu\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract the input of the last layer. These are the features.\n",
    "    embedding_size = list(model.children())[-1][1].in_features\n",
    "    num_images = len(train_dataset)\n",
    "    embeddings = np.zeros((num_images, embedding_size))\n",
    "\n",
    "    # Solution from url:\n",
    "    # https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch\n",
    "    new_model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "    # Not necessary since the model has already been moved and data gets moved later.\n",
    "    # # Put model into eval\n",
    "    # new_model.eval()\n",
    "    #\n",
    "    # # Sent model to device\n",
    "    # new_model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Temporarily turn off the gradient computation.\n",
    "        print(\"Evaluating the model for the training data\")\n",
    "        for batch_index, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            # Create embedding from retrieving the features from the last layer.\n",
    "            data = data.to(device)\n",
    "            output = new_model(data)\n",
    "\n",
    "            # Squeeze feature set down from 4D with 2 redundant dimensions to 2D.\n",
    "            # .cpu() moves the object from gpu to cpu memory, .numpy() creates a numpy object.\n",
    "            extracted_features = output.squeeze().cpu().numpy()\n",
    "\n",
    "            # Since we have chosen batch_size = 64 fill embeddings in this way\n",
    "            start = batch_index * train_loader.batch_size\n",
    "            finish = (batch_index + 1) * train_loader.batch_size\n",
    "            embeddings[start:finish] = extracted_features\n",
    "\n",
    "    np.save(path, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, path, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    # Load the data from the triplets.\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # Generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"P3/dataset\", transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '').split(\"\\\\\")[-1] for s in train_dataset.samples]\n",
    "    embeddings = np.load(path)\n",
    "\n",
    "    # Use standard normal normalisation\n",
    "    normalized_embeddings = (embeddings - np.mean(embeddings, axis=0)) / np.std(embeddings, axis=0)\n",
    "    # use between 0 and 1 normalization.\n",
    "    # normalized_embeddings = (embeddings - np.min(embeddings, axis=0)) / np.max(embeddings, axis=0)\n",
    "\n",
    "    # Create a dictionary that stores the embedding for each image. For this step it is important\n",
    "    # that the embeddings are generated in order, such that the embedding index matches the filename.\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = normalized_embeddings[i]\n",
    "\n",
    "    # Setup empty data lists.\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Use the individual embeddings to generate the features and labels for triplets\n",
    "    # Pick out a triplet t.\n",
    "    if train:\n",
    "        dataset_name = 'training'\n",
    "    else:\n",
    "        dataset_name = 'testing'\n",
    "\n",
    "    print(f'Loading the triplets for {dataset_name} dataset')\n",
    "    for t in tqdm(triplets):\n",
    "        # Access the dictionary to create a 3 part list that contains the image embedding for all 3 images.\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "\n",
    "        # Append the data to the list with arrays.\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader_from_np(X, y=None, train=True, batch_size=64, shuffle=True, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "\n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float),\n",
    "                                torch.from_numpy(y).type(torch.float))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, function):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Fully connected model with 3 x the amount of features per picture and binary output.\n",
    "        # For ResNet18 adjust to 512x3 features per picture.\n",
    "        # For ResNet50 adjust to 2048x3 features per picture.\n",
    "        # For EfficientNetB0 adjust to 1280x3 features per picture.\n",
    "        # For EfficientNetB3 adjust to 1536x3 features per picture.\n",
    "        # For EfficientNetB7 adjust to 2560x3 features per picture.\n",
    "        # For EfficientNetB8 2048x3\n",
    "        \n",
    "        self.fc1 = nn.Linear(4608, 3072)\n",
    "        self.fc2 = nn.Linear(3072, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "        self.output = nn.Linear(512, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(3072)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "\n",
    "        \n",
    "        self.function = function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.function(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.function(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.function(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.function(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = x.squeeze()\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, loss_function, optimizer, epochs, device, batch_size, split):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model\n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "\n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    validation_loss = []\n",
    "    losses = []\n",
    "    metric = BinaryAccuracy().to(device)\n",
    "\n",
    "    # Setup the lines to live plot the data. TODO: Fix this.\n",
    "    # hl, = plt.plot([0], [0])\n",
    "\n",
    "    # Introduce split between training and validation data with split\n",
    "    dataset = train_loader.dataset\n",
    "\n",
    "    val_split = split\n",
    "    train_split = 1 - split\n",
    "    len_train = int(len(dataset) * train_split)\n",
    "    len_val = len(dataset) - len_train\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [len_train, len_val])\n",
    "\n",
    "    # Initialize the minimum validation loss to infinity.\n",
    "    minimum_val_loss = np.inf\n",
    "\n",
    "    \n",
    "    # Apply the training procedure for the specified number of epochs and differentiate between validation and training.\n",
    "    train_val_loss = []\n",
    "\n",
    "    # Put the training and validation data in a loader.\n",
    "    \n",
    "    train_split_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=train_loader.batch_size, shuffle=True)\n",
    "    val_split_loader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=train_loader.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # Training the model\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        print('\\n')\n",
    "        print(f'Training the model in epoch {epoch}')\n",
    "        print('\\n')\n",
    "        for [X, y] in tqdm(train_split_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model.forward(X)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.detach()\n",
    "            \n",
    "            \n",
    "        train_loss = train_loss/len(train_split_loader)\n",
    "\n",
    "        # Validating the model\n",
    "        model.eval()\n",
    "        print('\\n')\n",
    "        print(f'Validating the model in epoch {epoch}')\n",
    "        print('\\n')\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            for [X_valid, y_valid] in tqdm(val_split_loader):\n",
    "                X_valid = X_valid.to(device)\n",
    "                y_valid = y_valid.to(device)\n",
    "                y_valid_pred = model.forward(X_valid)\n",
    "                \n",
    "                loss = loss_function(y_valid_pred, y_valid)\n",
    "                val_loss += loss\n",
    "                accuracy += metric(y_valid_pred, y_valid)\n",
    "            val_loss = val_loss/len(val_split_loader)\n",
    "            accuracy = accuracy/len(val_split_loader)\n",
    "        \n",
    "        print('\\n')\n",
    "        print(f'Train {epoch} loss: {train_loss}, validation loss: {val_loss}, accuracy: {accuracy}')\n",
    "        print('\\n')\n",
    "        train_val_loss.append((train_loss, val_loss))\n",
    "\n",
    "\n",
    "        ### We can use the validation data again to train. For this we need to define the best epoch model\n",
    "        if val_loss < minimum_val_loss:\n",
    "            minimum_val_loss = val_loss\n",
    "            min_val_state = model.state_dict()\n",
    "\n",
    "    # The model is trained on the train split. Take the best model and train it on the whole dataset.\n",
    "    print(\"Training the model with minimal validation loss on the whole dataset\")\n",
    "    model.load_state_dict(min_val_state)\n",
    "\n",
    "    # Define number of epochs for the final training\n",
    "    epochs_final = 5\n",
    "\n",
    "    # Define training procedure for the final training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    final_train_loss = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs_final)):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        print('\\n')\n",
    "        print(f'Final training the model in epoch {epoch}')\n",
    "        print('\\n')\n",
    "        for [X, y] in tqdm(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model.forward(X)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.detach()\n",
    "            \n",
    "            \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        final_train_loss.append(train_loss)\n",
    "        print('\\n')\n",
    "        print(f'Final train {epoch} loss: {train_loss}')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    return model, train_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and\n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "\n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():  # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        # Modification from the original\n",
    "        # predictions = np.vstack(predictions)\n",
    "        predictions = np.hstack(predictions)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"P3/results.txt\", predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # the device objectss indexes all available GPU's, since there is only one NVidea GPU in my pc it is cuda:0,\n",
    "    # or cuda() .\n",
    "    # use list(range(torch.cuda.device_count())) to find all avaible GPU's.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Used device: {device}')\n",
    "    # device = \"cpu\"\n",
    "\n",
    "    # Datasets\n",
    "    TRAIN_TRIPLETS = 'P3/train_triplets.txt'\n",
    "    TEST_TRIPLETS = 'P3/test_triplets.txt'\n",
    "\n",
    "    # Path to dataset\n",
    "    embedding_name = \"otto_embeddings_efficientnetb3.npy\"\n",
    "    Path = os.path\n",
    "    dir = Path.join(Path.dirname(__file__))\n",
    "    dir_path = Path.join(Path.dirname(__file__), \"dataset\")\n",
    "    path = Path.join(dir_path, embedding_name)\n",
    "\n",
    "    # Create model\n",
    "    model = Net(F.relu)\n",
    "    model.to(device)\n",
    "\n",
    "    # Setup parameters\n",
    "    batch_size = 100\n",
    "    num_work = 6\n",
    "    epochs = 15\n",
    "    split = 0.1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.BCELoss()  # nn.CrossEntropyLoss() only for 2 or more classes\n",
    "    test = True\n",
    "\n",
    "    # generate embedding for each image in the dataset\n",
    "    if (os.path.exists(path) == False):\n",
    "        generate_embeddings(path, batch_size=batch_size, num_work=num_work)\n",
    "\n",
    "    # Load the training and testing data\n",
    "    X, y = get_data(TRAIN_TRIPLETS, path=path)\n",
    "\n",
    "    # Create data loaders for the training and testing data\n",
    "    print('Generating loader for training data.')\n",
    "    train_loader = create_loader_from_np(X, y, train=True, batch_size=batch_size, num_workers=num_work)\n",
    "    print('Done generating loader for training data.')\n",
    "\n",
    "    # Define a model and train it\n",
    "    model, train_val_loss = train_model(model, train_loader,\n",
    "                                                 loss_function, optimizer,\n",
    "                                                 epochs, device, batch_size, split)\n",
    "\n",
    "    \n",
    "\n",
    "    # test the model on the test data\n",
    "    if test:\n",
    "        X_test, _ = get_data(TEST_TRIPLETS, train=False, path=path)\n",
    "        # For testing we use a larger batch size but adjust to pretrain model\n",
    "        test_loader = create_loader_from_np(X_test, train=False, batch_size=100, shuffle=False, num_workers=num_work)\n",
    "        test_model(model, test_loader)\n",
    "        print(\"Results saved to results.txt\")\n",
    "\n",
    "    # Plot the training and validation loss\n",
    "    plot_loss(train_val_loss, epochs)\n",
    "\n",
    "# What has been tried and yields similar results, batch sizes of 64\n",
    "# One layer, lr=0.01.\n",
    "# Two layers, lr=0.01 and 0.001 where the intermediate layer has 2056 nodes.\n",
    "\n",
    "# Batch sizes of 100\n",
    "# Three layers, lr=0.01, 0.001 intermediate layer has 3056 and 1024 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
